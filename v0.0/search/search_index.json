{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DataGEMS Data Workflow Orchestrator","text":"<p>This is the documentation site for the DataGEMS Data Workflow Orchestrator. The service is part of the wider DataGEMS platform. This is part of the overall platform documentation.</p> <p>The platform Data Workflow Orchestrator service acts as the horizontal data processing workflow solution for DataGEMS serving dataset onboarding, profiling and indexing tasks. </p> <p>You can use the menu options on the left to navigate through the available documentation. You may be interested to jump directly to an Architecture Overview, see the available Onboarding Material or see some Workflow Examples. You can find here the service code repository.</p> <p>For any questions, you can consult the FAQ, check if there is a relevant issue answering your question or contact the DataGEMS Help Desk.</p>"},{"location":"architecture/","title":"Service Architecture","text":"<p>This service is built using the Apache/Airflow tool. Apache Airflow is an open-source platform for developing, scheduling, and monitoring batch-oriented workflows. It offers an extensible Python framework that enables us to build workflows connecting with virtually any technology. A web-based UI helps us visualize, manage, and debug our workflows.</p>"},{"location":"architecture/#why-apache-airflow","title":"Why Apache Airflow","text":"<p>Apache Airflow was selected as the orchestration layer for this solution because it provides explicit, observable, and resilient workflow coordination across multiple services. The platform is particularly well-suited for environments where execution order, timing, and reliability are first-class architectural concerns. Rather than embedding orchestration logic inside individual services, Airflow centralizes workflow control while keeping services loosely coupled and independently deployable.</p>"},{"location":"architecture/#explicit-workflow-orchestration","title":"Explicit Workflow Orchestration","text":"<p>In Airflow, workflows are defined declaratively using \"Directed Acyclic Graphs\", in short DAGs. Inside a DAG, dependencies between execution steps are explicit, deterministic and inspectable. These individual execution pieces of work are called Tasks, arranged with dependencies and data flows taken into account.</p> <p></p> <p>A Dag specifies the dependencies between tasks, which defines the order in which to execute the tasks. Tasks describe what to do, be it fetching data, running analysis, triggering other systems, or more.</p>"},{"location":"architecture/#workflows-as-code","title":"Workflows as code","text":"<p>Airflow workflows are defined entirely in Python. This \"workflows as code\" approach brings several advantages:</p> <ul> <li>Dynamic: Pipelines are defined in code, enabling dynamic DAG generation and parameterization.</li> <li>Extensible: The Airflow framework includes a wide range of built-in operators and can be further extended to cover a wide variety of programming needs.</li> </ul>"},{"location":"architecture/#centralized-control-without-centralized-logic","title":"Centralized Control Without Centralized Logic","text":"<p>Airflow acts as a control plane, not a business logic engine:</p> <ul> <li>Services remain autonomous and focused on domain responsibilities</li> <li>Airflow coordinates when and under what conditions services interact</li> <li>No service needs awareness of downstream consumers or execution order</li> </ul> <p>This architecture reduces cross-service dependencies, improves service reusability and enables independent scaling and deployment.</p>"},{"location":"architecture/#built-in-reliability-and-failure-semantics","title":"Built-in Reliability and Failure Semantics","text":"<p>Airflow provides failure handling as a core architectural feature:</p> <ul> <li>Task-level retries with configurable backoff</li> <li>Timeouts and execution limits</li> <li>Partial re-runs without restarting from the beginning</li> </ul> <p>This allows the system to recover gracefully from transient service outages, avoid cascading failures and maintain predictable operational behavior.</p>"},{"location":"architecture/#strong-observability-and-operational-transparency","title":"Strong Observability and Operational Transparency","text":"<p>Airflow exposes execution state as part of its core model. DAGs and their tasks can be inspected visually, as well as logs and timestamps kept. Also, the historical audit trail of all workflow executions is kept.</p> <p>These features provide immediate insight into system behavior and even controlled manual intervention when required.</p> <p></p>"},{"location":"architecture/#techical-information","title":"Techical Information","text":"<p>Airflow's core components include the following, which form the backbone of its orchestration system:</p>"},{"location":"architecture/#scheduler","title":"Scheduler","text":"<p>The scheduler is the heart of Airflow. It continually monitors all DAGs and their tasks, triggering task instances once their dependencies are met. The scheduler is responsible for starting new DAG runs at the appropriate times (based on schedule or manual triggers) and determining which tasks are ready to run next. When a task is ready, the scheduler delegates its execution to an Executor, which will actually launch the task (possibly on a worker). The scheduler process runs indefinitely, polling the DAG definitions and the database for changes, and it orchestrates task execution in a continuous loop.</p>"},{"location":"architecture/#executor","title":"Executor","text":"<p>The executor is the mechanism by which the scheduler distributes tasks for execution. It is configured as part of the scheduler (not a separate daemon). Different executors determine how and where tasks run. We have used the KubernetesExecutor, which launches each task in its own Kubernetes pod for isolation and scalability.</p> <p>Regardless of executor type, the scheduler's job is to hand tasks to the executor, which then handles the low-level details of starting the task in the appropriate environment.</p>"},{"location":"architecture/#workers","title":"Workers","text":"<p>Workers are the processes (or containers) that actually execute the code of the tasks. In a simple setup using the LocalExecutor, the scheduler itself spawns worker processes internally to run tasks. In the distributed setup we have used, workers are separate processes (Kubernetes pods) that run on different machines. Workers continuously poll for tasks from the executor's queue and execute those tasks. Once finished, a worker will report the task's outcome back to the Airflow system (e.g. update task status in the database). Workers can be scaled out horizontally to run many tasks in parallel across multiple machines.</p>"},{"location":"architecture/#web-server","title":"Web Server","text":"<p>The web server is a FastAPI application that provides Airflow's user interface and RESTful APIs. It allows users to inspect DAGs, view task statuses, trigger runs, and debug workflows. The APIs provided are:</p> <ul> <li>an internal API for workers to report status or fetch information while running tasks,</li> <li>an internal API for the UI to dynamically fetch data (e.g. task updates) without direct database access,</li> <li>and the public Airflow REST API for users to programmatically manage the Airflow instance.</li> </ul> <p>The web server/API server does not execute DAG code; it simply displays information and handles user/worker requests. All sensitive operations (like running tasks) are handled by the scheduler and workers. The web server interacts with other components primarily through the metadata database or internal APIs. For security, in a distributed deployment the web server does not read DAG files directly - if a user views DAG code in the UI, it is fetched from the database (where the scheduler/DAG processor stored a serialized version of the DAG).</p>"},{"location":"architecture/#metadata-database","title":"Metadata Database","text":"<p>Airflow uses a persistent metadata database to store the state of the system. This database contains DAG definitions (serialized DAGs), task instance states (e.g. success, running, failed), DAG run records, scheduler metadata (like next run times), Airflow configurations, connections, variables, and history of past runs. Essentially, it is the single source of truth for the status of all workflows. Every component in Airflow interacts with this database:</p> <ul> <li>The scheduler adds DAG run and task instance entries and updates their states as tasks progress.</li> <li>Workers (or the tasks they run) update the database when tasks start, succeed, or fail (in newer versions this is done via the API server as an intermediary, rather than direct DB writes).</li> <li>The web server reads from the database to display current DAG and task statuses in the UI.</li> </ul> <p>Because of this central role, the metadata DB is a critical component; Airflow cannot run without it.</p>"},{"location":"architecture/#dag-files-and-dag-processor","title":"DAG Files and DAG Processor","text":"<p>Airflow DAGs are defined by Python files (DAG files) that users (DAG authors) write. These files need to be accessible to the Airflow system. The DAG processor is the component responsible for reading (parsing) these DAG files and storing their structure in the metadata DB for the scheduler to use. This means the DAG parsing is isolated from the main scheduler process, improving scheduler performance and security by clearly separating DAG code execution from scheduling logic. The DAG processor watches the DAG file directories (which can be local filesystems or object storage, depending on configuration), parses DAG definitions on change or at intervals, and saves a serialized version of each DAG to the database. This serialized DAG is what the scheduler actually uses to decide what tasks to run and when. In a multi-machine setup, DAG files must be distributed or synchronized to all places where they are needed (scheduler, any workers, and any triggerers) so that each component can access the latest DAG code. In our setup, this requirement is met by using NFS-mounted directories to ensure DAG files are consistently available across all Airflow components.</p>"},{"location":"architecture/#component-interaction-and-communication","title":"Component Interaction and Communication","text":"<p>In a running Airflow deployment, these components work together to orchestrate workflows. Each component has a well-defined role, and they communicate mainly through the metadata database and, in some cases, through an inter-process messaging system (for task execution). The diagram below (from the official Airflow documentation) illustrates a typical Airflow architecture in a distributed setting with the core components separated:</p> <p></p> <p>Each arrow in the figure represents a communication or data flow between components.</p>"},{"location":"architecture/#dag-execution-and-task-dependencies","title":"DAG Execution and Task Dependencies","text":"<p>Airflow manages workflow execution by enforcing the dependencies and order defined in each DAG. As mentioned above, a DAG is a directed acyclic graph where nodes are tasks and edges define dependencies (which task comes after which). These dependencies can be set in code (e.g. using task1 &gt;&gt; task2 or similar notation) and effectively create rules like \"task2 cannot run until task1 has finished successfully\". Here's how Airflow orchestrates tasks within a DAG, step by step:</p> <ol> <li> <p>DAG Parsing and Scheduling: When a DAG (workflow) is added or updated, the DAG processor or scheduler will load that DAG file, parse the DAG's tasks and dependencies, and save a serialized DAG representation in the metadata database. The scheduler constantly evaluates DAGs to see if a new DAG run should be started - for example, if a DAG is scheduled to run every day at 9 AM, the scheduler will create a new DAG run at that time (if one isn't already running or queued). If a DAG is triggered manually (via UI or API), the scheduler also creates a DAG run promptly. Each DAG run is an instance of the workflow (often corresponding to a specific execution date or data interval).</p> </li> <li> <p>Dependency Checking and Task Queuing: Once a DAG run is initiated, the scheduler examines all tasks in that DAG to decide which tasks are ready to run. A task is ready when all its upstream dependencies in the DAG have succeeded (or are skipped, depending on trigger rules) or if it has no upstream dependencies at all. For the first tasks in the DAG (those with no predecessors, or whose predecessors are all done in previous runs), the scheduler will immediately mark them as runnable. The scheduler then puts these task instances into a queue via the executor. In this context, \"queue\" could be any message broker, or, in our case, the Kubernetes API server (K8s executor launching pods). The scheduler records in the database that those tasks are queued and in what state.</p> </li> <li> <p>Task Execution on Workers: Worker processes pick up the queued tasks and begin executing them. Each task instance runs the user-defined code. During execution, the task instance will log its output and may report heartbeat signals to the scheduler to indicate it's still running (this helps detect if tasks hang or the process died).</p> </li> <li> <p>Task Completion and Result Reporting: When a task finishes (successfully or with failure), it needs to report its state so that Airflow knows it's done. For security reasons, the task/worker does not directly touch the database. Instead, the worker makes a call to the API server, using an authenticated internal API. The API server in turn writes the delegated information to the metadata database on behalf of the worker. Similarly, if the task needs to fetch connection credentials or other info stored in Airflow's DB (e.g., to get a password from a connection), the worker will call an endpoint on the API server, which will retrieve the data from the DB and return it to the task at runtime. This mechanism enhances security by preventing direct database access from arbitrary task code while still providing the information and persistence needed.</p> </li> <li> <p>Scheduler Monitoring and Dependency Resolution: The scheduler is continually running in the background, polling the database for task updates. As soon as a task changes state (for example, from \"running\" to \"success\"), the scheduler takes note. It will then check if that task's completion satisfies any dependencies for downstream tasks. In each DAG run, tasks that were waiting on the now-completed task may become eligible to run. The scheduler thus transitions those tasks to the \"queued\" state and sends them to the executor, as described in step 2. This event-driven scheduling continues until all tasks in the DAG have been completed or the DAG run is otherwise finished (e.g., failed or stopped).</p> </li> <li> <p>Handling Task Dependencies and Triggers: By default, the Airflow scheduler applies the all-success trigger rule, meaning a task is eligible to run only after all of its upstream tasks have completed successfully. Airflow also supports more advanced dependency semantics that allow DAG authors to model complex workflows. Tasks can be configured to run when any upstream task succeeds or fails by adjusting their trigger rules. DAGs may also include branching logic, where the outcome of one task determines which downstream path is followed, with non-selected paths being skipped automatically. In addition, Airflow supports time-based and external dependencies through sensors. Deferrable sensors offload their waiting logic to the triggerer, allowing worker slots to remain free while external conditions are being met. Regardless of the complexity of the DAG structure, Airflow enforces dependency correctness by tracking task states in the metadata database and evaluating each DAG run as a whole. A task will never run before its declared prerequisites are satisfied. If an upstream failure is not tolerated by a downstream task's trigger rule, the downstream task will be skipped or marked as failed according to that rule.</p> </li> <li> <p>Parallelism and Concurrency: Airflow can run multiple tasks at the same time as long as they are ready to run and resources allow it. Tasks in the same DAG that have no dependencies on each other (or whose dependencies are already resolved) may be executed in parallel on different workers. Also, multiple DAGs can run concurrently. The system respects configured limits such as maximum concurrency per DAG and global worker pool limits (pools allow you to limit how many tasks of certain type or certain resource usage run simultaneously). The scheduler will not queue more tasks than the system can handle as per these limits. This ensures a single DAG cannot overwhelm the whole system if configured with appropriate limits. Airflow's design, using a central scheduler, ensures that even in highly parallel scenarios, the dependencies and order of execution are correctly maintained.</p> </li> <li> <p>User Monitoring and Intervention: Throughout a DAG run, the Airflow Web UI provides visibility into the state of the workflow. The UI (via the API server) fetches the latest task statuses and DAG run information from the metadata DB and displays the DAG's graph or grid view showing which tasks are queued, running, succeeded, or failed. Users can click on tasks to see logs (which the web server retrieves from the log storage), check XCom messages, or mark tasks for retry. If needed, users can intervene via the UI or CLI - for example, they can clear a task (reset its state to let it run again), trigger a backfill or a new run, or pause a DAG to stop scheduling new runs. These actions all update states in the database or send signals to the scheduler, which will then act on them (e.g., if you clear a failed task, the scheduler will treat it as pending and reschedule it). The architecture ensures that user actions are propagated to the scheduler through the same channels (DB or internal APIs), so the scheduler remains the single point orchestrating the actual execution.</p> </li> </ol> <p>Airflow's handling of task orchestration is designed to ensure that task dependencies are honored and that workflows run in the intended order despite a distributed execution. The combination of the scheduler's central brain, the state tracking in the metadata database, and workers executing tasks allows Airflow to reliably manage complex DAGs. It's worth noting that Airflow does not guarantee all tasks of a DAG run on the same worker or machine - tasks will run wherever there is capacity (which is why passing data between tasks is done through XCom or external storage, not local files). This distributed nature provides flexibility and scalability: as our workflow load grows, we can add more workers or another scheduler for redundancy, and Airflow will continue to orchestrate tasks in a consistent manner.</p>"},{"location":"automations/","title":"Automations","text":"<p>A number of automations are available to facilitate the development, quality assurance, security, deployment, maintenance and onboarding of the service. Here we describe some that are directly, publicly available.</p>"},{"location":"automations/#documentation","title":"Documentation","text":"<p>A GitHub Action workflow is available to generate documentation available in the project in the format presented here. The action is triggered manually and can be executed against the head of the repository. The documentation generated is versioned and the documentation version is expected as input to the workflow. </p> <p>The documentation is build using the mkdocs library and specifically using the Material for mkdocs plugin. A number of additional tools are ustilized, such as mike to support versioning, and others.</p> <p>The documentation is generated and tagged with the provided version. It is uploaded to a dedicated documentation branch that is configured to be used as the base branch over which the repository GitHub Page presents its contents.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>The configuraiton options follow the available configuration files that Airflow supports.</p>"},{"location":"configuration/#secrets","title":"Secrets","text":"<p>Some of the configuration values contain sensitive data and should be treated differently. These may include secrets, connection strings, etc. It is suggested that, depending on the available infrastructure and tooling, the handling of these values is done separately from other configuration values.</p>"},{"location":"datastore/","title":"Data Stores","text":"<p>The service functions primarily as the data processing workflow horizontal solution for all the related DataGEMS flows. The data it stores include primarily workflow status, executions, etc.</p>"},{"location":"datastore/#relational-database","title":"Relational Database","text":"<p>The primary data store for the service is a PostgreSQL hosted relational database. </p> <p>The schema of the relational database is the one managed by the Airflow solution.</p>"},{"location":"datastore/#updates","title":"Updates","text":"<p>When updating the Airflow service to newer versions, any needed database updates are handled by the migration process of Airflow.</p>"},{"location":"deployment/","title":"Deployment","text":"<p>The service is part of the DataGEMS platform offered through an existing deployment, following the DataGEMS release and deployment procedures over a managed infrasrtucture. The purpose of this section is not to detail the deployment processes put in place by the DataGEMS team.</p>"},{"location":"deployment/#helm-charts","title":"Helm Charts","text":"<p>The service is offered as an existing Kubernetes Helm Chart.</p>"},{"location":"deployment/#configuration","title":"Configuration","text":"<p>In order for the service to operate properly, the needed configuration values must be set to match the environment that it must operate in. The needed configuration is described in the relevant Configuration section.</p>"},{"location":"deployment/#dependencies","title":"Dependencies","text":"<p>For the service to be able to operate, its underpinning services and dependnecies must be available and accessible. The Architecture section describes the ecosystem in which the service needs to operate. </p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#is-datagems-really-that-awsome","title":"Is DataGEMS really that awsome?","text":"<p>Yes, it is.</p>"},{"location":"license/","title":"License","text":"<pre><code>                          Apache License\n                    Version 2.0, January 2004\n                 http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li>Definitions.</li> </ol> <p>\"License\" shall mean the terms and conditions for use, reproduction,    and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by    the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all    other entities that control, are controlled by, or are under common    control with that entity. For the purposes of this definition,    \"control\" means (i) the power, direct or indirect, to cause the    direction or management of such entity, whether by contract or    otherwise, or (ii) ownership of fifty percent (50%) or more of the    outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity    exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,    including but not limited to software source code, documentation    source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical    transformation or translation of a Source form, including but    not limited to compiled object code, generated documentation,    and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or    Object form, made available under the License, as indicated by a    copyright notice that is included in or attached to the work    (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object    form, that is based on (or derived from) the Work and for which the    editorial revisions, annotations, elaborations, or other modifications    represent, as a whole, an original work of authorship. For the purposes    of this License, Derivative Works shall not include works that remain    separable from, or merely link (or bind by name) to the interfaces of,    the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including    the original version of the Work and any modifications or additions    to that Work or Derivative Works thereof, that is intentionally    submitted to Licensor for inclusion in the Work by the copyright owner    or by an individual or Legal Entity authorized to submit on behalf of    the copyright owner. For the purposes of this definition, \"submitted\"    means any form of electronic, verbal, or written communication sent    to the Licensor or its representatives, including but not limited to    communication on electronic mailing lists, source code control systems,    and issue tracking systems that are managed by, or on behalf of, the    Licensor for the purpose of discussing and improving the Work, but    excluding communication that is conspicuously marked or otherwise    designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity    on behalf of whom a Contribution has been received by Licensor and    subsequently incorporated within the Work.</p> <ol> <li> <p>Grant of Copyright License. Subject to the terms and conditions of    this License, each Contributor hereby grants to You a perpetual,    worldwide, non-exclusive, no-charge, royalty-free, irrevocable    copyright license to reproduce, prepare Derivative Works of,    publicly display, publicly perform, sublicense, and distribute the    Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of    this License, each Contributor hereby grants to You a perpetual,    worldwide, non-exclusive, no-charge, royalty-free, irrevocable    (except as stated in this section) patent license to make, have made,    use, offer to sell, sell, import, and otherwise transfer the Work,    where such license applies only to those patent claims licensable    by such Contributor that are necessarily infringed by their    Contribution(s) alone or by combination of their Contribution(s)    with the Work to which such Contribution(s) was submitted. If You    institute patent litigation against any entity (including a    cross-claim or counterclaim in a lawsuit) alleging that the Work    or a Contribution incorporated within the Work constitutes direct    or contributory patent infringement, then any patent licenses    granted to You under this License for that Work shall terminate    as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the    Work or Derivative Works thereof in any medium, with or without    modifications, and in Source or Object form, provided that You    meet the following conditions:</p> </li> </ol> <p>(a) You must give any other recipients of the Work or        Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices        stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works        that You distribute, all copyright, patent, trademark, and        attribution notices from the Source form of the Work,        excluding those notices that do not pertain to any part of        the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its        distribution, then any Derivative Works that You distribute must        include a readable copy of the attribution notices contained        within such NOTICE file, excluding those notices that do not        pertain to any part of the Derivative Works, in at least one        of the following places: within a NOTICE text file distributed        as part of the Derivative Works; within the Source form or        documentation, if provided along with the Derivative Works; or,        within a display generated by the Derivative Works, if and        wherever such third-party notices normally appear. The contents        of the NOTICE file are for informational purposes only and        do not modify the License. You may add Your own attribution        notices within Derivative Works that You distribute, alongside        or as an addendum to the NOTICE text from the Work, provided        that such additional attribution notices cannot be construed        as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and    may provide additional or different license terms and conditions    for use, reproduction, or distribution of Your modifications, or    for any such Derivative Works as a whole, provided Your use,    reproduction, and distribution of the Work otherwise complies with    the conditions stated in this License.</p> <ol> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,    any Contribution intentionally submitted for inclusion in the Work    by You to the Licensor shall be under the terms and conditions of    this License, without any additional terms or conditions.    Notwithstanding the above, nothing herein shall supersede or modify    the terms of any separate license agreement you may have executed    with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade    names, trademarks, service marks, or product names of the Licensor,    except as required for reasonable and customary use in describing the    origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or    agreed to in writing, Licensor provides the Work (and each    Contributor provides its Contributions) on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or    implied, including, without limitation, any warranties or conditions    of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A    PARTICULAR PURPOSE. You are solely responsible for determining the    appropriateness of using or redistributing the Work and assume any    risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,    whether in tort (including negligence), contract, or otherwise,    unless required by applicable law (such as deliberate and grossly    negligent acts) or agreed to in writing, shall any Contributor be    liable to You for damages, including any direct, indirect, special,    incidental, or consequential damages of any character arising as a    result of this License or out of the use or inability to use the    Work (including but not limited to damages for loss of goodwill,    work stoppage, computer failure or malfunction, or any and all    other commercial damages or losses), even if such Contributor    has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing    the Work or Derivative Works thereof, You may choose to offer,    and charge a fee for, acceptance of support, warranty, indemnity,    or other liability obligations and/or rights consistent with this    License. However, in accepting such obligations, You may act only    on Your own behalf and on Your sole responsibility, not on behalf    of any other Contributor, and only if You agree to indemnify,    defend, and hold each Contributor harmless for any liability    incurred by, or claims asserted against, such Contributor by reason    of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <p>To apply the Apache License to your work, attach the following    boilerplate notice, with the fields enclosed by brackets \"[]\"    replaced with your own identifying information. (Don't include    the brackets!)  The text should be enclosed in the appropriate    comment syntax for the file format. We also recommend that a    file or class name and description of purpose be included on the    same \"printed page\" as the copyright notice for easier    identification within third-party archives.</p> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"maintenance/","title":"Maintenance","text":"<p>The service is part of the DataGEMS platform offered through an existing deployment, following the DataGEMS release and deployment procedures over a managed infrasrtucture along with the maintenance activities that are scheduled within the platform. The purpose of this section is not to detail the maintenance activities put in place by the DataGEMS team.</p>"},{"location":"maintenance/#healthchecks","title":"Healthchecks","text":"<p>The service observability documentation describes healthcheck endpoints that can be used to track the status of the service. </p> <p>An example of a healthcheck response that returns 200 OK for healthy state is: <pre><code>{\n  \"metadatabase\":{\n    \"status\":\"healthy\"\n  },\n  \"scheduler\":{\n    \"status\":\"healthy\",\n    \"latest_scheduler_heartbeat\":\"2018-12-26 17:15:11+00:00\"\n  },\n  \"triggerer\":{\n    \"status\":\"healthy\",\n    \"latest_triggerer_heartbeat\":\"2018-12-26 17:16:12+00:00\"\n  },\n  \"dag_processor\":{\n    \"status\":\"healthy\",\n    \"latest_dag_processor_heartbeat\":\"2018-12-26 17:16:12+00:00\"\n  }\n}\n</code></pre></p>"},{"location":"maintenance/#verions-updates","title":"Verions &amp; Updates","text":"<p>The service follows the versioning and update scheme that Airflow supports.</p>"},{"location":"maintenance/#backups","title":"Backups","text":"<p>All state persisted by the service is maintained in the relational database as described in the respective datastores section.</p> <p>To keep backups of the state, the respective utilities must be scheduled to run in a consistent manner. </p>"},{"location":"maintenance/#troubleshooting","title":"Troubleshooting","text":"<p>Troubleshooting is primarily done through the logging mechanisms that are available within Airflow. These offer log reports on individual tasks and flows as they are executed by the Airflow engine.</p>"},{"location":"onboarding/","title":"Onboarding Material","text":"<p>This section contains some references and material that will assist users and integrators onboarding process.</p>"},{"location":"onboarding/#references","title":"References","text":"<p>The definite guide for integrators remains the Airflow documentation and API reference.</p> <p>Airflow is an open source project and the codebase is available under Apache License Version 2.0 in the project's GitHub code repository.</p> <p>Custom workflows applying for the DataGEMS use cases are available under Apache License Version 2.0 in our GitHub code repository.</p> <p>More information on the workflows supported for the DataGEMS data management needs can be found at the Workflow Overview section.</p> <p>For an overall view of the service, you can go through the Architecture.</p> <p>For any questions, you can consult the FAQ.</p> <p>If you are facing a problem, check if there is a relevant issue answering your question.</p> <p>You can always contact us through the DataGEMS Help Desk.</p>"},{"location":"onboarding/#examples","title":"Examples","text":"<p>You can find descriptions and an overview of the offered workflows the Workflow Overview section.</p>"},{"location":"onboarding/#tutorials","title":"Tutorials","text":"<p>You can find useful material and onboarding guidelines in our social channels bellow, as well as our platform documentation.</p>"},{"location":"onboarding/#keep-in-touch","title":"Keep in touch","text":"<p>Make sure to follow the DataGEMS channels to get the latest news and references:</p> <ul> <li>GitHub</li> <li>Instagram</li> <li>X</li> <li>YoutTube</li> <li>LinkedIn</li> <li>Facebook</li> </ul>"},{"location":"qa/","title":"Quality Assurance","text":"<p>Key aspects of the Quality Assurance checklist that DataGEMS services must pass have been defined in the processes and documents governing the platform development and quality assurance. </p> <p>For the case of DataGEMS Data Workflow Orchestrator we rely on the the Airflow workflow solution which is a well established and widely used open source project.</p>"},{"location":"workflow-dataset-onboarding/","title":"Dataset Onboarding Workflow","text":"<p>The Dataset Onboarding Workflow provides a unified and controlled process for registering and ingesting datasets into the platform, regardless of their original storage location. The workflow accepts dataset metadata together with one or more dataset locations and guides the dataset through a well-defined lifecycle that separates physical data handling from logical dataset management.</p>"},{"location":"workflow-dataset-onboarding/#input","title":"Input","text":"<p>In in the initial step, the workflow is provided with the following information:</p> Parameter Type Special Format Mandatory Description id string uuid yes A globally unique identifier for the dataset. This identifier is used consistently throughout the workflow to associate metadata, staged files, and lifecycle events with the same dataset entity. code string no An optional short or human-friendly code used to reference the dataset. This may correspond to an internal catalog code or business identifier. name string no The human-readable name of the dataset. Used for display, discovery, and identification purposes. description string no A detailed textual description of the dataset contents, scope, and purpose. Helps users understand what the dataset represents and how it can be used. headline string no A short, high-level summary or tagline describing the dataset. Typically used in user interfaces or dataset listings. fields_of_science array of strings no A list of scientific or domain classifications associated with the dataset. Used for categorization, filtering, and discovery. languages array of strings no The languages represented in the dataset content. This may refer to the language of textual data or metadata. keywords string no Free-form keywords associated with the dataset. Used to improve searchability and discoverability. countries array of strings no publishedUrl string uri no A public or external URL where the dataset is published or described. This may point to documentation, landing pages, or external repositories. citeAs string no A recommended citation string for referencing the dataset in publications. Used to support proper attribution and academic reuse. conformsTo string no A reference to a standard, schema, or specification that the dataset conforms to. This may include formal data models or community standards. license string no The license under which the dataset is distributed. Defines usage rights, redistribution terms, and legal constraints. size number integer no dataLocations array of objects {Kind: string, location: string} yes A list of locations describing where the dataset is physically stored or accessed. Each entry specifies the type of location (e.g. file system, remote service, database), and the corresponding access reference (path, URI, connection, etc.). This parameter is mandatory and drives staging, branching, and loading behavior in the workflow. version string no The version identifier of the dataset. Used to distinguish between different releases or revisions of the same dataset. mime_type string no The MIME type representing the datase's format (e.g. text/csv, application/json). Helps downstream systems interpret and process the data correctly. date_published string date no The publication date of the dataset. Used for provenance tracking and lifecycle management. userId string no An identifier representing the user or actor initiating the onboarding process. Used for attribution, auditing, and ownership tracking. <p>All the fields except dataLocations contain metadata information. The dataLocations contains the information where the Dataset is located. The following locations are supported:</p> <ul> <li>File: The dataset is stored inside a file somewhere in the mounted filesystem.</li> <li>Http: The dataset can be accessed via a URI.</li> <li>Ftp: The dataset is stored inside an FTP server. The location should be an FTP URI.</li> <li>Remote: TODO</li> <li>Staged: The dataset is already staged.</li> <li>Database: The dataset is stored inside a relational database.</li> </ul>"},{"location":"workflow-dataset-onboarding/#tasks","title":"Tasks","text":"<p>This workflow follows the diagram that will be explained below:</p> <p></p>"},{"location":"workflow-dataset-onboarding/#stage-dataset-files","title":"Stage Dataset Files","text":"<p>This task ensures that all dataset files are available in a controlled, local staging area before the dataset is registered or loaded. Because datasets may originate from heterogeneous storage systems, this step acts as a normalization phase: regardless of where the data comes from, downstream tasks interact with a consistent file-based representation whenever applicable. The task iterates over all declared dataset locations and applies the appropriate handling strategy for each one.</p> <p>If the dataset is already available on the local filesystem, already staged by a previous process, stored in a relational database, or referenced by a location type that does not require materialization, then no action is performed and the location is passed through unchanged.</p> <p>If the dataset is hosted externally (for example via HTTP or FTP), the dataset files are retrieved from the remote source, stored in a predefined staging directory associated with the dataset and assigned unique filenames to avoid collisions or ambiguity.</p> <p>The task produces a normalized list of dataset locations that reflects the effective access points for the dataset:</p> <ul> <li>unchanged locations for sources that do not require staging,</li> <li>staged file locations for externally retrieved datasets.</li> </ul> <p>This output is used by subsequent tasks to register the dataset metadata and decide whether the dataset should be physically loaded or only registered.</p> <p>If any dataset location cannot be accessed, retrieved, or staged successfully, the workflow fails at this step. This prevents partial or inconsistent dataset onboarding.</p>"},{"location":"workflow-dataset-onboarding/#register-dataset","title":"Register Dataset","text":"<p>This task registers the dataset and its metadata in the central Data Model Management system, making the dataset discoverable, traceable, and addressable within the platform. The task uses the metadata provided at workflow start, together with the effective dataset locations produced by the staging step, to create a canonical dataset representation. The task returns the response from the Data Model Management system, which is primarily informational; downstream control flow is determined by the dataset location types rather than this response.</p> <p>If the registration request fails, the workflow terminates immediately. No dataset loading is attempted unless the dataset has been successfully registered.</p>"},{"location":"workflow-dataset-onboarding/#choose-if-the-dataset-will-be-loaded","title":"Choose if the Dataset will be loaded","text":"<p>This step determines whether the workflow should proceed with physically loading the dataset or stop after registration. The decision is based on the types of dataset locations associated with the dataset: If the dataset is stored in a relational database, the workflow ends after registration. Otherwise, the workflow continues to the loading step. By explicitly separating registration from loading, the workflow supports multiple dataset lifecycle patterns while maintaining a consistent onboarding process.</p>"},{"location":"workflow-dataset-onboarding/#load-dataset","title":"Load Dataset","text":"<p>This task performs the physical ingestion of the dataset into downstream systems after it has been successfully registered. Loading is only executed for datasets that require file-based processing. Database-resident datasets and other non-loadable sources are explicitly excluded by the preceding decision step. This operation does not redefine dataset metadata; it acts upon the dataset entity created during registration.</p> <p>The task returns the response from the Data Model Management system which is primarily informational and does not affect further workflow execution.</p> <p>If loading fails, the workflow fails at this stage. The dataset remains registered but is not marked as loaded, allowing the issue to be diagnosed and retried without re-registration.</p>"},{"location":"workflow-dataset-profiling/","title":"Dataset Profiling Workflow","text":"<p>This workflow orchestrates the profiling lifecycle of a dataset, from profile generation to metadata integration and cleanup. It produces two complementary profiles\u2014 light and heavy\u2014 and ensures that both are correctly persisted before releasing profiling resources.</p>"},{"location":"workflow-dataset-profiling/#input","title":"Input","text":"<p>Initially, the workflow is provided with the following information:</p> Parameter Type Special Format Mandatory Description id string uuid yes A globally unique identifier of the dataset to be profiled. This identifier links profiling results to the correct dataset entity in downstream systems. code string no An optional short or human-friendly code used to reference the dataset. Primarily intended for cataloging or internal identification. name string no The human-readable name of the dataset. Used for identification and traceability throughout the profiling lifecycle. description string no A detailed textual description of the dataset. Provides contextual information that may be associated with profiling results. headline string no A concise, high-level summary of the dataset. Typically used for display purposes in user interfaces. fields_of_science array of strings no A list of scientific or domain classifications associated with the dataset. Used for categorization and analytical context. languages array of strings no The languages represented in the dataset content. Relevant for interpreting profiling metrics, especially for textual data. keywords string no Free-form keywords associated with the dataset. Support discoverability and semantic interpretation of profiling results. countries array of strings no A list of countries relevant to the dataset. May indicate geographic coverage, origin, or regulatory context. publishedUrl string uri no A public or external URL where the dataset is described or published. May reference documentation, landing pages, or external repositories. citeAs string no A recommended citation string for the dataset. Used to support attribution and reuse in publications. conformsTo string no A reference to a standard, schema, or specification that the dataset adheres to. Provides structural or semantic context for profiling outputs. license string no The license under which the dataset is distributed. Defines legal and usage constraints relevant to downstream consumers. size number integer no An approximate size indicator for the dataset. May represent file size, record count, or another agreed-upon metric. version string no The version identifier of the dataset. Used to distinguish profiling results across different dataset revisions. mime_type string no The MIME type describing the dataset's format (e.g. text/csv, application/json). Helps the profiling system select appropriate analysis strategies. date_published string date no The publication date of the dataset. Used for provenance tracking and temporal context. userId string no An identifier representing the user or actor who initiated the profiling workflow. Used for attribution, auditing, and traceability. data_store_kind 0 or 1 yes An indicator of how the dataset is physically stored. This value determines how the profiler accesses the dataset (e.g. file-based vs. database-backed) and influences profiling execution behavior. archivedAt string path yes The path or location where the dataset is archived and accessed for profiling. This serves as the authoritative source from which profiling jobs read the dataset."},{"location":"workflow-dataset-profiling/#tasks","title":"Tasks","text":"<p>The workflow follows the diagram below, which illustrates parallel execution, synchronization points, and cleanup:</p> <p></p>"},{"location":"workflow-dataset-profiling/#trigger-profiles","title":"Trigger Profiles","text":"<p>The execution is split in two parallel task instance queues, one for light profiling and one for heavy, as shown in the diagram. The first level of tasks ran in parallel get the corresponding authorization token and communicate with the Profiler service to trigger the generation of the profile. This parallelization allows lightweight metadata extraction and more intensive analysis to proceed independently and efficiently.</p>"},{"location":"workflow-dataset-profiling/#check-if-the-profiles-are-ready","title":"Check if the Profiles are ready","text":"<p>Once profiling jobs have been triggered, each branch enters a polling phase. In this phase the workflow periodically checks the status of the corresponding profiling job. Execution is paused between checks to avoid unnecessary load. The workflow advances only when the profile has reached a \"ready\" state. If a profiling job fails, or is cleaned up prematurely, the workflow terminates with a failure, ensuring that incomplete or invalid profiles are never propagated. This step guarantees that downstream tasks only operate on fully generated and valid profiling results.</p>"},{"location":"workflow-dataset-profiling/#fetch-profiles","title":"Fetch Profiles","text":"<p>After a profile is reported as ready, the workflow retrieves its contents from the Profiler service. For each profile (light and heavy) the complete profiling result is fetched with the data preserved in its original structured form, with no transformation or interpretation applied. This separation ensures that profiling generation and profiling consumption remain decoupled.</p>"},{"location":"workflow-dataset-profiling/#update-data-management","title":"Update Data Management","text":"<p>Once profiling data has been retrieved, the workflow updates the dataset representation in the Data Model Management system. The light and heavy profiles are handled as distinct but complementary updates, allowing consumers to benefit from different levels of detail. Successful completion of this step means the dataset is now profile-aware within the platform.</p>"},{"location":"workflow-dataset-profiling/#profile-cleanup","title":"Profile Cleanup","text":"<p>After both profiling branches have completed successfully, the workflow converges into a final cleanup step. Cleanup is intentionally executed after all profiling data has been safely persisted, ensuring no loss of information.</p>"}]}